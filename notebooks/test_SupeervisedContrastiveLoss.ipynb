{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 19:32:30.589973: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-03 19:32:30.649744: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-03 19:32:30.649792: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-03 19:32:30.652140: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-03 19:32:30.665247: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-03 19:32:31.671725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_50246/3769053210.py:7: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n",
      "Setting memory growth to True for GPU:  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 19:32:33.375040: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 19:32:33.540565: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 19:32:33.540856: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 19:32:33.675181: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 19:32:33.675437: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 19:32:33.675620: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 19:32:33.675752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 46872 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:07:00.0, compute capability: 8.6\n",
      "2024-01-03 19:32:33.678267: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 19:32:33.678492: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 19:32:33.678636: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Activation\n",
    "import numpy as np\n",
    "\n",
    "# verify if GPU is available\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "# set memory growth to true\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Setting memory growth to True for GPU: \", physical_devices[0])\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# dont display much info of tensorflow\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manta shape:  (9587, 320, 320)\n",
      "xiris shape:  (9587, 320, 320)\n",
      "y shape:  (9587, 2)\n",
      "y shape:  (9587,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data\n",
    "manta_path = \"/home/vm/SSL_Project_1/data/processed/bag_2023-07-04_15-23-48/_manta.npy\"\n",
    "xiris_path = \"/home/vm/SSL_Project_1/data/processed/bag_2023-07-04_15-23-48/_xiris.npy\"\n",
    "y_path = \"/home/vm/SSL_Project_1/data/processed/bag_2023-07-04_15-23-48/_y.npy\"\n",
    "feats_path = \"/home/vm/SSL_Project_1/data/processed/bag_2023-07-04_15-23-48/_feats.npy\"\n",
    "\n",
    "# load numpy arrays and display shapes\n",
    "manta = np.load(manta_path)\n",
    "xiris = np.load(xiris_path)\n",
    "y = np.load(y_path)\n",
    "print(\"manta shape: \", manta.shape)\n",
    "print(\"xiris shape: \", xiris.shape)\n",
    "print(\"y shape: \", y.shape) # laser power and velocity\n",
    "\n",
    "#feats = np.load(feats_path)\n",
    "#print(\"feats shape: \", feats.shape)\n",
    "y = y[:, 0] # only use laser power\n",
    "print(\"y shape: \", y.shape)\n",
    "\n",
    "# normalize y\n",
    "y = y / np.max(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in y:  [0.18181818 0.45454545 0.72727273 1.        ]\n",
      "y encoded:  [0. 0. 0. ... 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "# unique values in y\n",
    "y_unique = np.unique(y)\n",
    "print(\"unique values in y: \", y_unique)\n",
    "\n",
    "# encode y as integers based on unique values\n",
    "y_encoded = np.zeros(y.shape)\n",
    "for i in range(len(y_unique)):\n",
    "    y_encoded[y == y_unique[i]] = i\n",
    "print(\"y encoded: \", y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle manta and xiris with y_encoded\n",
    "idx = np.arange(len(y))\n",
    "np.random.shuffle(idx)\n",
    "manta = manta[idx]\n",
    "xiris = xiris[idx]\n",
    "y_encoded = y_encoded[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (15338, 320, 320)\n",
      "X_test shape:  (3836, 320, 320)\n",
      "y_train shape:  (15338, 1)\n",
      "y_test shape:  (3836, 1)\n"
     ]
    }
   ],
   "source": [
    "# get first 80% of data as training data\n",
    "train_split = int(0.8 * len(y))\n",
    "manta_train = manta[:train_split]\n",
    "xiris_train = xiris[:train_split]\n",
    "y_train = y_encoded[:train_split]\n",
    "\n",
    "\n",
    "# get last 20% of data as test data\n",
    "manta_test = manta[train_split:]\n",
    "xiris_test = xiris[train_split:]\n",
    "y_test = y_encoded[train_split:]\n",
    "\n",
    "# reshape manta and xiris\n",
    "X_train = np.concatenate((manta_train, xiris_train), axis=0)\n",
    "X_test = np.concatenate((manta_test, xiris_test), axis=0)\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "\n",
    "# reshape y\n",
    "y_train = np.concatenate((y_train, y_train), axis=0)\n",
    "y_test = np.concatenate((y_test, y_test), axis=0)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # concatenate the two inputs (manta and xiris) along rows\\nx = np.concatenate((manta, xiris), axis=0)\\ny = np.concatenate((y_encoded, y_encoded), axis=0)\\nprint(\"x shape: \", x.shape)\\nprint(\"y shape: \", y.shape)\\n\\n# split data into train and test (manta as input and y as output) with shuffle as true\\nfrom sklearn.model_selection import train_test_split\\n\\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=True, random_state=42)\\nprint(\"x_train shape: \", x_train.shape)\\nprint(\"y_train shape: \", y_train.shape)\\nprint(\"x_test shape: \", x_test.shape)\\nprint(\"y_test shape: \", y_test.shape)\\n '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # concatenate the two inputs (manta and xiris) along rows\n",
    "x = np.concatenate((manta, xiris), axis=0)\n",
    "y = np.concatenate((y_encoded, y_encoded), axis=0)\n",
    "print(\"x shape: \", x.shape)\n",
    "print(\"y shape: \", y.shape)\n",
    "\n",
    "# split data into train and test (manta as input and y as output) with shuffle as true\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"x_test shape: \", x_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 320, 320, 1)]     0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 194688)            4800      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               24920192  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24924992 (95.08 MB)\n",
      "Trainable params: 24924992 (95.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 19:33:03.585458: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 19:33:03.585829: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 19:33:03.585938: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 19:33:03.586162: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 19:33:03.586269: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 19:33:03.586355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46872 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def create_encoder(input_shape=(320, 320, 1)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(16, (3, 3), activation='relu')(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# add projection head\n",
    "def add_projection_head(input_shape, encoder, embedding_dim):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    # add dense and dropout\n",
    "    # add batch normalization\n",
    "    #features = BatchNormalization()(features)\n",
    "    #features = Dense(512, activation='relu')(features)\n",
    "    #features = Dropout(0.5)(features)\n",
    "    outputs = Dense(embedding_dim, activation='relu')(features)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (320, 320, 1)\n",
    "embedding_dim = 128\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "learning_rate = 0.0001\n",
    "temperature = 0.05\n",
    "\n",
    "encoder = create_encoder()\n",
    "encoder_with_projection_head = add_projection_head(input_shape, encoder, embedding_dim)\n",
    "encoder_with_projection_head.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vm/laser/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "\n",
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        # Compute logits\n",
    "        logits = tf.divide(\n",
    "            tf.matmul(\n",
    "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 320, 320, 1)]     0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 194688)            4800      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               24920192  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24924992 (95.08 MB)\n",
      "Trainable params: 24924992 (95.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 19:33:22.341368: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-01-03 19:33:22.471749: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-03 19:33:22.908245: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-03 19:33:25.078098: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f7eb0ea88d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-03 19:33:25.078146: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-01-03 19:33:25.085361: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1704310405.268584   50302 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 18s 63ms/step - loss: 3.5299 - val_loss: 3.3375\n",
      "Epoch 2/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 3.1024 - val_loss: 3.1168\n",
      "Epoch 3/20\n",
      "192/192 [==============================] - 8s 41ms/step - loss: 2.9625 - val_loss: 2.9505\n",
      "Epoch 4/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 2.9001 - val_loss: 2.9246\n",
      "Epoch 5/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 2.8702 - val_loss: 2.8984\n",
      "Epoch 6/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 2.8498 - val_loss: 2.8696\n",
      "Epoch 7/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 2.8316 - val_loss: 2.8736\n",
      "Epoch 8/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 2.8318 - val_loss: 2.8810\n",
      "Epoch 9/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 2.8134 - val_loss: 2.8969\n",
      "Epoch 10/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 2.8097 - val_loss: 2.8749\n",
      "Epoch 11/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 2.8108 - val_loss: 2.8889\n",
      "Epoch 12/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 2.8088 - val_loss: 2.8652\n",
      "Epoch 13/20\n",
      "192/192 [==============================] - 8s 40ms/step - loss: 2.8043 - val_loss: 2.8711\n",
      "Epoch 14/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 2.8008 - val_loss: 2.8768\n",
      "Epoch 15/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 2.7992 - val_loss: 2.8790\n",
      "Epoch 16/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 2.7996 - val_loss: 2.8774\n",
      "Epoch 17/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 2.7989 - val_loss: 2.8684\n",
      "Epoch 18/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 2.8095 - val_loss: 2.8836\n",
      "Epoch 19/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 2.8061 - val_loss: 2.8810\n",
      "Epoch 20/20\n",
      "192/192 [==============================] - 8s 40ms/step - loss: 2.8048 - val_loss: 2.8800\n"
     ]
    }
   ],
   "source": [
    "encoder_with_projection_head.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=SupervisedContrastiveLoss(temperature),\n",
    ")\n",
    "\n",
    "encoder_with_projection_head.summary()\n",
    "\n",
    "history = encoder_with_projection_head.fit(\n",
    "    x=X_train, y=y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "240/240 [==============================] - 6s 22ms/step - loss: 0.9592 - sparse_categorical_accuracy: 0.6994\n",
      "Epoch 2/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.6188 - sparse_categorical_accuracy: 0.8562\n",
      "Epoch 3/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.4792 - sparse_categorical_accuracy: 0.8901\n",
      "Epoch 4/20\n",
      "240/240 [==============================] - 5s 20ms/step - loss: 0.4013 - sparse_categorical_accuracy: 0.9122\n",
      "Epoch 5/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.3498 - sparse_categorical_accuracy: 0.9308\n",
      "Epoch 6/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.3130 - sparse_categorical_accuracy: 0.9430\n",
      "Epoch 7/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.2846 - sparse_categorical_accuracy: 0.9501\n",
      "Epoch 8/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.2620 - sparse_categorical_accuracy: 0.9564\n",
      "Epoch 9/20\n",
      "240/240 [==============================] - 5s 20ms/step - loss: 0.2429 - sparse_categorical_accuracy: 0.9596\n",
      "Epoch 10/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.2267 - sparse_categorical_accuracy: 0.9615\n",
      "Epoch 11/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.2127 - sparse_categorical_accuracy: 0.9630\n",
      "Epoch 12/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.2003 - sparse_categorical_accuracy: 0.9652\n",
      "Epoch 13/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.1896 - sparse_categorical_accuracy: 0.9670\n",
      "Epoch 14/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.1795 - sparse_categorical_accuracy: 0.9684\n",
      "Epoch 15/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.1705 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 16/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.1618 - sparse_categorical_accuracy: 0.9708\n",
      "Epoch 17/20\n",
      "240/240 [==============================] - 5s 20ms/step - loss: 0.1545 - sparse_categorical_accuracy: 0.9719\n",
      "Epoch 18/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.1472 - sparse_categorical_accuracy: 0.9727\n",
      "Epoch 19/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.1407 - sparse_categorical_accuracy: 0.9740\n",
      "Epoch 20/20\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.1342 - sparse_categorical_accuracy: 0.9745\n"
     ]
    }
   ],
   "source": [
    "dropout_rate = 0.2\n",
    "hidden_units = 128\n",
    "num_classes = 4\n",
    "num_epochs = 20\n",
    "def create_classifier(encoder, trainable=False):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    #features = layers.Dropout(dropout_rate)(features)\n",
    "    #features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "classifier = create_classifier(encoder, trainable=False)\n",
    "\n",
    "history = classifier.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model with x_test and y_test...\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.1364 - sparse_categorical_accuracy: 0.9718\n",
      "loss:  0.136428564786911\n",
      "acc:  0.9718456864356995\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with x_test and y_test\n",
    "loss, acc = classifier.evaluate(x=X_test, y=y_test)\n",
    "print(\"loss: \", loss)\n",
    "print(\"acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 1s 16ms/step - loss: 0.0651 - sparse_categorical_accuracy: 0.9838\n"
     ]
    }
   ],
   "source": [
    "# test model with only manta images -> manta_test with first alph of y_test\n",
    "y_test_manta = y_test[:len(y_test)//2]\n",
    "#manta_test = X_test[:len(X_test)//2]\n",
    "\n",
    "accuracy = classifier.evaluate(x=manta_test, y=y_test_manta)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'distance_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 76\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreduce_mean(y_true \u001b[38;5;241m*\u001b[39m square_pred \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y_true) \u001b[38;5;241m*\u001b[39m (margin_square_laser_power \u001b[38;5;241m+\u001b[39m margin_square_velocity))\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Create the distance layer\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m distance \u001b[38;5;241m=\u001b[39m \u001b[43mdistance_layer\u001b[49m(image1_network\u001b[38;5;241m.\u001b[39moutput, image2_network\u001b[38;5;241m.\u001b[39moutput, laser_power1, velocity1, laser_power2, velocity2)\n\u001b[1;32m     77\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39m[image1_network\u001b[38;5;241m.\u001b[39minput, image2_network\u001b[38;5;241m.\u001b[39minput, laser_power1, velocity1, laser_power2, velocity2], outputs\u001b[38;5;241m=\u001b[39mdistance)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'distance_layer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming you have your images and features preprocessed and stored in numpy arrays\n",
    "# image1, image2, laser_power, velocity\n",
    "\n",
    "# Define the base network architecture\n",
    "def create_base_network():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(32))\n",
    "    return model\n",
    "\n",
    "# Create two instances of the base network\n",
    "base_network = create_base_network()\n",
    "image1_network = models.Model(inputs=base_network.input, outputs=base_network.output)\n",
    "image2_network = models.Model(inputs=base_network.input, outputs=base_network.output)\n",
    "\n",
    "# Define the distance layer\n",
    "class DistanceLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, image1, image2, laser_power1, velocity1, laser_power2, velocity2):\n",
    "        image_distance = tf.reduce_sum(tf.square(image1 - image2), axis=-1)\n",
    "        laser_power_distance = tf.square(laser_power1 - laser_power2)\n",
    "        velocity_distance = tf.square(velocity1 - velocity2)\n",
    "        return image_distance + laser_power_distance + velocity_distance\n",
    "    \n",
    "\n",
    "class ContrastiveLossLaserPower(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, y_true, y_pred, laser_power1, laser_power2):\n",
    "        # Calculate the margin as a function of laser power\n",
    "        margin_laser_power = tf.abs(laser_power1 - laser_power2)\n",
    "\n",
    "        square_pred = tf.square(y_pred)\n",
    "        margin_square_laser_power = tf.square(tf.maximum(margin_laser_power - y_pred, 0))\n",
    "        return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square_laser_power)\n",
    "\n",
    "class ContrastiveLossVelocity(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, y_true, y_pred, velocity1, velocity2):\n",
    "        # Calculate the margin as a function of velocity\n",
    "        margin_velocity = tf.abs(velocity1 - velocity2)\n",
    "\n",
    "        square_pred = tf.square(y_pred)\n",
    "        margin_square_velocity = tf.square(tf.maximum(margin_velocity - y_pred, 0))\n",
    "        return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square_velocity)\n",
    "    \n",
    "class ContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, y_true, y_pred, laser_power1, velocity1, laser_power2, velocity2):\n",
    "        # Calculate the margins as functions of laser power and velocity\n",
    "        margin_laser_power = tf.abs(laser_power1 - laser_power2)\n",
    "        margin_velocity = tf.abs(velocity1 - velocity2)\n",
    "\n",
    "        square_pred = tf.square(y_pred)\n",
    "        margin_square_laser_power = tf.square(tf.maximum(margin_laser_power - y_pred, 0))\n",
    "        margin_square_velocity = tf.square(tf.maximum(margin_velocity - y_pred, 0))\n",
    "        return tf.reduce_mean(y_true * square_pred + (1 - y_true) * (margin_square_laser_power + margin_square_velocity))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the distance layer\n",
    "distance = distance_layer(image1_network.output, image2_network.output, laser_power1, velocity1, laser_power2, velocity2)\n",
    "model = models.Model(inputs=[image1_network.input, image2_network.input, laser_power1, velocity1, laser_power2, velocity2], outputs=distance)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='contrastive_loss')  # You need to define 'contrastive_loss'\n",
    "\n",
    "# Train the model\n",
    "# model.fit([image1, image2, laser_power, velocity], labels, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.contrastive_loss_laser_power = ContrastiveLossLaserPower()\n",
    "        self.contrastive_loss_velocity = ContrastiveLossVelocity()\n",
    "\n",
    "    def call(self, y_true, y_pred, laser_power1, laser_power2, velocity1, velocity2):\n",
    "        loss_laser_power = self.contrastive_loss_laser_power(y_true, y_pred, laser_power1, laser_power2)\n",
    "        loss_velocity = self.contrastive_loss_velocity(y_true, y_pred, velocity1, velocity2)\n",
    "        return self.alpha * loss_laser_power + (1 - self.alpha) * loss_velocity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m         margin_square_velocity \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msquare(tf\u001b[38;5;241m.\u001b[39mmaximum(margin_velocity \u001b[38;5;241m-\u001b[39m y_pred, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreduce_mean(y_true \u001b[38;5;241m*\u001b[39m square_pred \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y_true) \u001b[38;5;241m*\u001b[39m margin_square_velocity)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m[ContrastiveLossLaserPower(), ContrastiveLossVelocity()])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "class ContrastiveLossLaserPower(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, y_true, y_pred, laser_power1, laser_power2):\n",
    "        # Calculate the margin as a function of laser power\n",
    "        margin_laser_power = tf.abs(laser_power1 - laser_power2)\n",
    "\n",
    "        square_pred = tf.square(y_pred)\n",
    "        margin_square_laser_power = tf.square(tf.maximum(margin_laser_power - y_pred, 0))\n",
    "        return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square_laser_power)\n",
    "\n",
    "class ContrastiveLossVelocity(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, y_true, y_pred, velocity1, velocity2):\n",
    "        # Calculate the margin as a function of velocity\n",
    "        margin_velocity = tf.abs(velocity1 - velocity2)\n",
    "\n",
    "        square_pred = tf.square(y_pred)\n",
    "        margin_square_velocity = tf.square(tf.maximum(margin_velocity - y_pred, 0))\n",
    "        return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square_velocity)\n",
    "    \n",
    "model.compile(optimizer='adam', loss=[ContrastiveLossLaserPower(), ContrastiveLossVelocity()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
