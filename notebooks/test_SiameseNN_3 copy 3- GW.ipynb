{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 23:02:29.385042: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-08 23:02:29.459626: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-08 23:02:29.459668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-08 23:02:29.462300: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-08 23:02:29.476293: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-08 23:02:30.343770: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-08 23:02:31.855665: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-08 23:02:31.916285: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-08 23:02:31.916488: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-08 23:02:31.918418: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-08 23:02:31.918576: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-08 23:02:31.918666: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-08 23:02:32.008577: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-08 23:02:32.008737: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-08 23:02:32.008846: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-08 23:02:32.008933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46872 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting memory growth to True for GPU:  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Physical GPUs:  1 Logical GPUs:  1\n"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "# dont display much info of tensorflow\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any level you prefer\n",
    "\n",
    "# limit gpu memory usage only as much as needed\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "        print(\"Setting memory growth to True for GPU: \", gpu)\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(\"Physical GPUs: \", len(gpus), \"Logical GPUs: \", len(logical_gpus))\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "from tensorflow.keras import layers, models, Input, Model\n",
    "from tensorflow.keras.layers import Lambda, Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Activation, GlobalAveragePooling2D, Concatenate, Add, AveragePooling2D\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manta shape:  (9587, 320, 320)\n",
      "xiris shape:  (9587, 320, 320)\n",
      "y shape:  (9587, 2)\n",
      "y shape:  (9587,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data\n",
    "manta_path = \"/home/vm/SSL_Project_1/data/processed/bag_2023-07-04_15-23-48/_manta.npy\"\n",
    "xiris_path = \"/home/vm/SSL_Project_1/data/processed/bag_2023-07-04_15-23-48/_xiris.npy\"\n",
    "y_path = \"/home/vm/SSL_Project_1/data/processed/bag_2023-07-04_15-23-48/_y.npy\"\n",
    "feats_path = \"/home/vm/SSL_Project_1/data/processed/bag_2023-07-04_15-23-48/_feats.npy\"\n",
    "\n",
    "# load numpy arrays and display shapes\n",
    "manta = np.load(manta_path)\n",
    "xiris = np.load(xiris_path)\n",
    "y = np.load(y_path)\n",
    "print(\"manta shape: \", manta.shape)\n",
    "print(\"xiris shape: \", xiris.shape)\n",
    "print(\"y shape: \", y.shape) # laser power and velocity\n",
    "\n",
    "#feats = np.load(feats_path)\n",
    "#print(\"feats shape: \", feats.shape)\n",
    "y = y[:, 0] # only use laser power\n",
    "print(\"y shape: \", y.shape)\n",
    "\n",
    "# normalize y\n",
    "y = y / np.max(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in y:  [0.18181818 0.45454545 0.72727273 1.        ]\n",
      "y encoded:  [0. 0. 0. ... 3. 3. 3.]\n",
      "y encoded type:  <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# unique values in y\n",
    "y_unique = np.unique(y)\n",
    "print(\"unique values in y: \", y_unique)\n",
    "\n",
    "# encode y as integers based on unique values\n",
    "y_encoded = np.zeros(y.shape)\n",
    "for i in range(len(y_unique)):\n",
    "    y_encoded[y == y_unique[i]] = i\n",
    "print(\"y encoded: \", y_encoded)\n",
    "# change to int\n",
    "y_encoded = y_encoded.astype(int)\n",
    "\n",
    "# print typr of y_encoded\n",
    "print(\"y encoded type: \", type(y_encoded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pairs for image 0/9587, Completed 0%\n",
      "Creating pairs for image 1000/9587, Completed 10%\n",
      "Creating pairs for image 2000/9587, Completed 20%\n",
      "Creating pairs for image 3000/9587, Completed 31%\n",
      "Creating pairs for image 4000/9587, Completed 41%\n",
      "Creating pairs for image 5000/9587, Completed 52%\n",
      "Creating pairs for image 6000/9587, Completed 62%\n",
      "Creating pairs for image 7000/9587, Completed 73%\n",
      "Creating pairs for image 8000/9587, Completed 83%\n",
      "Creating pairs for image 9000/9587, Completed 93%\n",
      "pairs shape:  (19174, 2, 320, 320)\n",
      "labels shape:  (19174, 2)\n",
      "X_train shape: (15339, 2, 320, 320)  y_train shape: (15339, 1) \n",
      "X_test shape: (3835, 2, 320, 320)  y_test shape: (3835, 1) \n",
      "max of y_train:  1\n",
      "min of y_train:  0\n",
      "max of y_test:  1\n",
      "min of y_test:  0\n"
     ]
    }
   ],
   "source": [
    "# crate pairs\n",
    "def create_pairs(manta, xiris, y_encoded):\n",
    "    # set seed\n",
    "    np.random.seed(42)\n",
    "        \n",
    "    pairs = []\n",
    "    labels = []\n",
    "    binary_labels = []\n",
    "    \n",
    "    # Getting the indices of each class\n",
    "    numclasses = len(np.unique(y_encoded))\n",
    "    idx = [np.where(y_encoded==i)[0] for i in range(numclasses)]\n",
    "\n",
    "    for idxA in range(len(y_encoded)):\n",
    "        # grab the current image and label belonging to the current iteration\n",
    "        currentImage = manta[idxA]\n",
    "        label1 = y_encoded[idxA]\n",
    "\n",
    "        # randomly pick an image that belongs to the same class label\n",
    "        idxB = np.random.choice(idx[label1])\n",
    "        posImage = xiris[idxB]\n",
    "\n",
    "        # prepare a positive pair and update the images and labels lists, respectively\n",
    "        pairs.append([currentImage, posImage])\n",
    "        labels.append([label1, label1])\n",
    "        binary_labels.append([0])\n",
    "\n",
    "        # grab the indices for each of the class labels not equal to the current label\n",
    "        negIdx = np.where(y_encoded != label1)[0]\n",
    "        \n",
    "        # randomly pick an image corresponding to a label not equal to the current label\n",
    "        idxC = np.random.choice(negIdx)\n",
    "        label2 = y_encoded[idxC]\n",
    "        negImage = xiris[idxC]\n",
    "        \n",
    "        # prepare a negative pair of images and update our lists\n",
    "        pairs.append([currentImage, negImage])\n",
    "        labels.append([label1, label2])\n",
    "        binary_labels.append([1])\n",
    "\n",
    "        if idxA % 1000 == 0:\n",
    "            print(f\"Creating pairs for image {idxA}/{len(y_encoded)}, Completed {int(idxA/len(y_encoded)*100)}%\")\n",
    "    \n",
    "    return np.array(pairs), np.array(labels), np.array(binary_labels)      \n",
    "                                                                                             \n",
    "# create pairs\n",
    "pairs, labels, binary_labels = create_pairs(manta, xiris, y_encoded)\n",
    "print(\"pairs shape: \", pairs.shape)\n",
    "print(\"labels shape: \", labels.shape)\n",
    "\n",
    "# split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(pairs, binary_labels, test_size=0.2, random_state=42, shuffle=True)\n",
    "print(f\"X_train shape: {X_train.shape} \",f\"y_train shape: {y_train.shape} \")\n",
    "print(f\"X_test shape: {X_test.shape} \",f\"y_test shape: {y_test.shape} \")\n",
    "\n",
    "# max and min of y_train and y_test\n",
    "print(\"max of y_train: \", np.max(y_train))\n",
    "print(\"min of y_train: \", np.min(y_train))\n",
    "\n",
    "print(\"max of y_test: \", np.max(y_test))\n",
    "print(\"min of y_test: \", np.min(y_test)) \n",
    "\n",
    "\n",
    "#del X_train, X_test, y_train, y_test\n",
    "del pairs, labels, binary_labels, manta, xiris, y, y_encoded, y_unique\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from sklearn.model_selection import train_test_split\\n\\nX_train2, X_test2, y_train2, y_test2 = train_test_split(pairs, labels, test_size=0.2, random_state=42, shuffle=True)\\n\\ndel pairs, labels '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(pairs, labels, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "del pairs, labels \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(pairs, labels, to_show=6, num_col=3, predictions=None, test=False):\n",
    "    \"\"\"Creates a plot of pairs and labels, and prediction if it's test dataset.\n",
    "\n",
    "    Arguments:\n",
    "        pairs: Numpy Array, of pairs to visualize, having shape\n",
    "               (Number of pairs, 2, 28, 28).\n",
    "        to_show: Int, number of examples to visualize (default is 6)\n",
    "                `to_show` must be an integral multiple of `num_col`.\n",
    "                 Otherwise it will be trimmed if it is greater than num_col,\n",
    "                 and incremented if if it is less then num_col.\n",
    "        num_col: Int, number of images in one row - (default is 3)\n",
    "                 For test and train respectively, it should not exceed 3 and 7.\n",
    "        predictions: Numpy Array of predictions with shape (to_show, 1) -\n",
    "                     (default is None)\n",
    "                     Must be passed when test=True.\n",
    "        test: Boolean telling whether the dataset being visualized is\n",
    "              train dataset or test dataset - (default False).\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    num_row = to_show // num_col if to_show // num_col != 0 else 1\n",
    "\n",
    "    # `to_show` must be an integral multiple of `num_col`\n",
    "    #  we found num_row and we have num_col\n",
    "    #  to increment or decrement to_show\n",
    "    #  to make it integral multiple of `num_col`\n",
    "    #  simply set it equal to num_row * num_col\n",
    "    to_show = num_row * num_col\n",
    "\n",
    "    # Plot the images\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(5, 5))\n",
    "    for i in range(to_show):\n",
    "        # If the number of rows is 1, the axes array is one-dimensional\n",
    "        if num_row == 1:\n",
    "            ax = axes[i % num_col]\n",
    "        else:\n",
    "            ax = axes[i // num_col, i % num_col]\n",
    "\n",
    "        ax.imshow(tf.keras.layers.concatenate([pairs[i][0], pairs[i][1]], axis=1), cmap=\"gray\")\n",
    "        ax.set_axis_off()\n",
    "        if test:\n",
    "            ax.set_title(\"True: {} | Pred: {:.5f}\".format(labels[i], predictions[i][0]))\n",
    "        else:\n",
    "            ax.set_title(\"Label: {}\".format(labels[i]))\n",
    "    if test:\n",
    "        plt.tight_layout(rect=(0, 0, 1.9, 1.9), w_pad=0.0)\n",
    "    else:\n",
    "        plt.tight_layout(rect=(0, 0, 1.5, 1.5))\n",
    "    plt.show()\n",
    "\n",
    "# visualize pairs\n",
    "#visualize(X_train2, y_train2, to_show=4, num_col=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss_with_margin(margin=1):\n",
    "    \"\"\"Provides 'contrastive_loss' an enclosing scope with variable 'margin'.\n",
    "\n",
    "    Arguments:\n",
    "        margin: Integer, defines the baseline for distance for which pairs\n",
    "                should be classified as dissimilar. - (default is 1).\n",
    "\n",
    "    Returns:\n",
    "        'contrastive_loss' function with data ('margin') attached.\n",
    "    \"\"\"\n",
    "\n",
    "    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
    "    #                         true_value * square( max(margin-prediction, 0) ))\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        \"\"\"Calculates the contrastive loss.\n",
    "\n",
    "        Arguments:\n",
    "            y_true: List of labels, each label is of type float32.\n",
    "            y_pred: List of predictions of same length as of y_true,\n",
    "                    each label is of type float32.\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing contrastive loss as floating point value.\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure that y_true is of type float32\n",
    "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "\n",
    "        square_pred = tf.square(y_pred)\n",
    "        margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "        return tf.reduce_mean((1 - y_true) * square_pred + (y_true) * margin_square)\n",
    "\n",
    "    return contrastive_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(input_shape=(320, 320, 1)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv2D(16, (3, 3), activation='relu')(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# add projection head\n",
    "def add_projection_head(input_shape, encoder, embedding_dim):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    outputs = Dense(embedding_dim, activation='relu')(features)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def contrastive_loss_2(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    \n",
    "    x =  tf.cast(y_true[:,0], dtype=tf.float32)\n",
    "    #print(\"x: \", x)\n",
    "    y =  tf.cast(y_true[:,1], dtype=tf.float32)\n",
    "    #print(\"y: \", y)\n",
    "    z = tf.abs(tf.subtract(x, y))\n",
    "    #print(\"z: \", z)\n",
    "    # using tf less to construct binary labels\n",
    "    y_b = tf.cast(tf.less(z, 1), tf.float32)\n",
    "\n",
    "    margin = 0.5\n",
    "    square_pred = tf.square(y_pred)\n",
    "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "    return tf.reduce_mean((1 - y_b) * square_pred + (y_b) * margin_square)\n",
    "\n",
    "# get first 4 values of y_train2\n",
    "#y_true2 = y_train2[:4, :]\n",
    "\n",
    "# test contrastive_loss_2\n",
    "y_true = np.array([[0, 0], [2, 1], [0, 3], [3, 3]])\n",
    "y_pred = np.array([0.09, 0.85, 0.95, 0.92])\n",
    "\n",
    "#print(contrastive_loss_2(y_true2, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # from https://github.com/keras-team/keras/blob/v3.0.2/keras/metrics/accuracy_metrics.py#L18 \\nfrom keras import backend\\nfrom keras import ops\\nfrom keras.losses.loss import squeeze_to_same_rank\\n\\ndef binary_accuracy(y_true, y_pred, threshold=0.5):\\n    y_true = ops.convert_to_tensor(y_true)\\n    y_pred = ops.convert_to_tensor(y_pred)\\n    y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\\n    threshold = ops.cast(threshold, y_pred.dtype)\\n    y_pred = ops.cast(y_pred > threshold, y_true.dtype)\\n    return ops.mean(\\n        ops.cast(ops.equal(y_true, y_pred), dtype=backend.floatx()),\\n        axis=-1,\\n    )\\n '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # from https://github.com/keras-team/keras/blob/v3.0.2/keras/metrics/accuracy_metrics.py#L18 \n",
    "from keras import backend\n",
    "from keras import ops\n",
    "from keras.losses.loss import squeeze_to_same_rank\n",
    "\n",
    "def binary_accuracy(y_true, y_pred, threshold=0.5):\n",
    "    y_true = ops.convert_to_tensor(y_true)\n",
    "    y_pred = ops.convert_to_tensor(y_pred)\n",
    "    y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n",
    "    threshold = ops.cast(threshold, y_pred.dtype)\n",
    "    y_pred = ops.cast(y_pred > threshold, y_true.dtype)\n",
    "    return ops.mean(\n",
    "        ops.cast(ops.equal(y_true, y_pred), dtype=backend.floatx()),\n",
    "        axis=-1,\n",
    "    )\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy_with_threshold(y_true, y_pred, threshold=0.5): # WORKS!!!\n",
    "    # Ensure the predicted values are between 0 and 1\n",
    "    y_pred = tf.clip_by_value(y_pred, 0, 1)\n",
    "\n",
    "    # Convert the predicted values to binary (0 or 1) using the specified threshold\n",
    "    y_pred_binary = tf.cast(tf.greater_equal(y_pred, threshold), tf.float32)\n",
    "\n",
    "    # Calculate the binary accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_true, y_pred_binary), tf.float32))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 23:03:18.919319: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def binary_accuracy_with_threshold_2(y_true, y_pred, threshold=0.4):\n",
    "    # Ensure that y_true is of type float32\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    \n",
    "    # Ensure the predicted values are between 0 and 1\n",
    "    y_1 = tf.cast(y_true[:,0], tf.float32)\n",
    "    y_2 = tf.cast(y_true[:,1], tf.float32)\n",
    "    \n",
    "    # y_true_b is 0 if y_1 and y_2 are equal, 1 otherwise\n",
    "    # give a margin of 0.25\n",
    "    #y_true_b = tf.cast(tf.less(tf.abs(tf.subtract(y_1, y_2)), 0.25), tf.float32)\n",
    "    y_true_b = tf.cast(tf.not_equal(y_1, y_2), tf.float32)\n",
    "\n",
    "    # Ensure the predicted values are between 0 and 1\n",
    "    y_pred = tf.clip_by_value(y_pred, 0, 1)\n",
    "    # Convert the predicted values to binary (0 or 1) using the specified threshold\n",
    "    y_pred_binary = tf.cast(tf.greater_equal(y_pred, threshold), tf.float32)\n",
    "\n",
    "    # Calculate the binary accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_true_b, y_pred_binary), tf.float32))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# test accuracy_2\n",
    "y_true = np.array([[0, 0], [2, 1], [0, 3], [3, 3]])\n",
    "y_pred = np.array([0.1, 0.85, 0.95, 0.3])\n",
    "\n",
    "print(binary_accuracy_with_threshold_2(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_1:  tf.Tensor([0. 2. 0. 3.], shape=(4,), dtype=float32)\n",
      "y_2:  tf.Tensor([0. 1. 3. 3.], shape=(4,), dtype=float32)\n",
      "y_true_b:  tf.Tensor([0. 1. 1. 0.], shape=(4,), dtype=float32)\n",
      "tf.Tensor(0.25375003, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def contrastive_loss_with_margin_2(y_true, y_pred):\n",
    "\n",
    "    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
    "    #                         true_value * square( max(margin-prediction, 0) ))\n",
    "    #def contrastive_loss(y_true, y_pred):\n",
    "    margin=1\n",
    "    # Ensure that y_true is of type float32\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    \n",
    "    # Ensure the predicted values are between 0 and 1\n",
    "    y_1 = y_true[:,0]\n",
    "    y_2 = y_true[:,1]\n",
    "    \n",
    "    print(\"y_1: \", y_1)\n",
    "    print(\"y_2: \", y_2)\n",
    "    \n",
    "    # y_true_b is 0 if y_1 and y_2 are equal, 1 otherwise\n",
    "    y_true_b = tf.cast(tf.not_equal(y_true[:,0], y_true[:,1]), tf.float32)\n",
    "\n",
    "    \n",
    "    # print y_true_b\n",
    "    print(\"y_true_b: \", y_true_b)\n",
    "\n",
    "    square_pred = tf.square(y_pred)\n",
    "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "    return tf.reduce_mean((1 - y_true_b) * square_pred + (y_true_b) * margin_square)\n",
    "\n",
    "    #return contrastive_loss\n",
    "\n",
    "y_true = np.array([[0, 0], [2, 1], [0, 3], [3, 3]])\n",
    "y_pred = np.array([0.1, 0.95, 0.95, 1])\n",
    "\n",
    "\n",
    "# test contrastive_loss_with_margin_2\n",
    "print(contrastive_loss_with_margin_2(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true:  tf.Tensor([0. 1. 1. 0.], shape=(4,), dtype=float32)\n",
      "y_pred:  tf.Tensor([0.1  0.95 0.95 1.  ], shape=(4,), dtype=float32)\n",
      "tf.Tensor(0.25375003, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "        \"\"\"Calculates the contrastive loss.\n",
    "\n",
    "        Arguments:\n",
    "            y_true: List of labels, each label is of type float32.\n",
    "            y_pred: List of predictions of same length as of y_true,\n",
    "                    each label is of type float32.\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing contrastive loss as floating point value.\n",
    "        \"\"\"\n",
    "        margin=1\n",
    "        # cast y_true to float32 and y_pred to float32\n",
    "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "        # print y_true and y_pred\n",
    "        print(\"y_true: \", y_true)\n",
    "        print(\"y_pred: \", y_pred)\n",
    "\n",
    "        square_pred = tf.square(y_pred)\n",
    "        margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "        return tf.reduce_mean((1 - y_true) * square_pred + (y_true) * margin_square)\n",
    "\n",
    "y_true_b = np.array([0, 1, 1, 0])\n",
    "y_pred = np.array([0.1, 0.95, 0.95, 1])\n",
    "\n",
    "# test contrastive_loss\n",
    "print(contrastive_loss(y_true_b, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (320, 320, 1)\n",
    "embedding_dim= 128\n",
    "batch_size = 64\n",
    "epochs = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "encoder = create_encoder(input_shape)\n",
    "encoder_with_projection_head = add_projection_head(input_shape, encoder, embedding_dim)\n",
    "\n",
    "manta = Input(shape=input_shape)\n",
    "xiris = Input(shape=input_shape)\n",
    "manta_encoded = encoder_with_projection_head(manta)\n",
    "xiris_encoded = encoder_with_projection_head(xiris)\n",
    "distance = tf.abs(manta_encoded - xiris_encoded)\n",
    "output = tf.keras.layers.Dense(1, activation=\"linear\")(distance)\n",
    "siamese_net = tf.keras.Model(inputs=[manta, xiris], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the contrastive loss using the two embeddings produced by the Siamese Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.accuracy_tracker = tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        \n",
    "        # print data shape\n",
    "        X, y = data  # Unpack the data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.siamese_network(X) # Forward pass\n",
    "            # compute loss\n",
    "            loss = self.contrastive_loss(y_true=y, y_pred=y_pred)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.accuracy_tracker.update_state(y, y_pred)\n",
    "        return {\"loss\": self.loss_tracker.result(), \"accuracy\": self.accuracy_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # model.evaluate() stores the losses and metrics in a list\n",
    "        \n",
    "        # Unpack the data\n",
    "        X, y = data\n",
    "        \n",
    "        # Compute predictions\n",
    "        y_pred = self.siamese_network(X, training=False)\n",
    "        # The loss is computed on the test set\n",
    "        loss = self.contrastive_loss(y_true=y, y_pred=y_pred)\n",
    "        #acc is the binary accuracy\n",
    "        self.accuracy_tracker.update_state(y, y_pred)\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        \n",
    "        return {\"loss\": self.loss_tracker.result(), \"accuracy\": self.accuracy_tracker.result()}\n",
    "\n",
    "    def contrastive_loss(self, y_true, y_pred):\n",
    "        \"\"\"Calculates the constrastive loss.\"\"\"\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "\n",
    "        square_pred = tf.square(y_pred)\n",
    "        margin_square = tf.square(tf.maximum(self.margin - y_pred, 0))\n",
    "        return tf.reduce_mean((1 - y_true) * square_pred + (y_true) * margin_square)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker, self.accuracy_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 23:03:42.882470: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-01-08 23:03:43.075259: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-08 23:03:45.857362: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f430dfb5600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-08 23:03:45.857420: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-01-08 23:03:45.873770: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1704755026.094196  385110 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 27s 107ms/step - loss: 0.3235 - accuracy: 0.8338 - val_loss: 0.0946 - val_accuracy: 0.8947\n",
      "Epoch 2/4\n",
      "192/192 [==============================] - 14s 72ms/step - loss: 0.0751 - accuracy: 0.9154 - val_loss: 0.0715 - val_accuracy: 0.9169\n",
      "Epoch 3/4\n",
      "192/192 [==============================] - 13s 69ms/step - loss: 0.0609 - accuracy: 0.9365 - val_loss: 0.0599 - val_accuracy: 0.9358\n",
      "Epoch 4/4\n",
      "192/192 [==============================] - 13s 67ms/step - loss: 0.0479 - accuracy: 0.9545 - val_loss: 0.0476 - val_accuracy: 0.9511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f460c55d330>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create siamese model\n",
    "siamese_model = SiameseModel(siamese_net)\n",
    "# compile model\n",
    "siamese_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "# fit model\n",
    "siamese_model.fit(\n",
    "    x=[X_train[:, 0], X_train[:, 1]], \n",
    "    y=y_train,#[y_train2[:,0],  y_train2[:,1]], \n",
    "    batch_size=batch_size, \n",
    "    epochs=4,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del siamese_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0452 - accuracy: 0.9570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04515353962779045, 0.9569752216339111]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model\n",
    "siamese_model.evaluate(x=[X_test[:, 0], X_test[:, 1]], y=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model siamese_net\n",
    "#del siamese_net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
